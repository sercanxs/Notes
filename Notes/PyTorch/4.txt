Neural Network classification with PyTorch

Classification is a problem of predicting whether something is one thing or another (there can be multiple things as the options).

Book version of this notebook - https://www.learnpytorch.io/02_pytorch_classification/
All other resources - https://github.com/mrdbourke/pytorch-deep-learning
Stuck? Ask a question - https://github.com/mrdbourke/pytorch-deep-learning/discussions


Make classification data and get it ready

import sklearn
from sklearn.datasets import make_circles

# Make 1000 samples
n_samples = 1000

# Create circles                       --> Make a large circle containing a smaller circle in 2d.
X, y = make_circles(n_samples,         --> 1000 veri ürettik.
                    noise=0.03,        --> Standard deviation of Gaussian noise added to the data.
                    random_state=42)   --> random seed ile aynı görevi yapıyor.
                                           Ekstaradan faktor de ayarlanabilir. default'u 0.8 dir. yani içerideki dairenin çapı dışarıdaki
                                           dairenin 5'te 4'üdür.
     

len(X), len(y)
     
(1000, 1000)

print(f"First 5 samples of X:\n {X[:5]}")
print(f"First 5 samples of y:\n {y[:5]}")
     
First 5 samples of X:
 [[ 0.75424625  0.23148074]
 [-0.75615888  0.15325888]
 [-0.81539193  0.17328203]
 [-0.39373073  0.69288277]
 [ 0.44220765 -0.89672343]]
First 5 samples of y:
 [1 1 1 1 0]                --> y değişkeni bir ve sıfırlardan oluşur. 1 değerlerine karşılık gelen X verileri dış daireyi 0 değerlerine
                                denk gelen X verileri iç daireyi oluşturur.

# Make DataFrame of circle data
import pandas as pd                    --> pandas kullandık
circles = pd.DataFrame({"X1": X[:, 0], 
                        "X2": X[:, 1],
                        "label": y})   --> key value girdik.
circles.head(10)                              
     
              X1	      X2    label
0	0.754246	0.231481	1
1	-0.756159	0.153259	1
2	-0.815392	0.173282	1
3	-0.393731	0.692883	1
4	0.442208	-0.896723	0
5	-0.479646	0.676435	1
6	-0.013648	0.803349	1
7	0.771513	0.147760	1
8	-0.169322	-0.793456	1
9	-0.121486	1.021509	0


circles.label.value_counts() 
     
1    500
0    500
Name: label, dtype: int64


# Visualize, visualize, visualize
import matplotlib.pyplot as plt
plt.scatter(x=X[:, 0],             --> x eksenine X değişkeninin yukarıdaki X1 kısmını koyduk.
            y=X[:, 1],             --> y eksenine X değişkeninin yukarıdaki X2 kısmını koyduk.
            c=y,                   --> c'ye ise y değişkenini label olsun diye koyduk. Yani y değerlerinin 1 olduğu noktalarda
                                       X değerleri bir renk alırken, y değerlerinin 0 olduğu noktalarda X değerleri başka renk alacak.
            cmap=plt.cm.RdYlBu);   --> RedYellowBlue'yi seçtik. Dış çember red olurken iç çember blue oldu.
     

Note: The data we're working with is often referred to as a toy dataset, a dataset that is small enough to experiment but still sizeable enough to practice the fundamentals.

-----

Check input and output shapes

X.shape, y.shape
     
((1000, 2), (1000,))

X
     
array([[ 0.75424625,  0.23148074],
       [-0.75615888,  0.15325888],
       [-0.81539193,  0.17328203],
       ...,
       [-0.13690036, -0.81001183],
       [ 0.67036156, -0.76750154],
       [ 0.28105665,  0.96382443]])

View the first example of features and labels
X_sample = X[0]
y_sample = y[0]

print(f"Values for one sample of X: {X_sample} and the same for y: {y_sample}")
print(f"Shapes for one sample of X: {X_sample.shape} and the same for y: {y_sample.shape}")
     
Values for one sample of X: [0.75424625 0.23148074] and the same for y: 1
Shapes for one sample of X: (2,) and the same for y: ()


Turn data into tensors and create train and test splits

import torch
torch.__version__
     
'1.10.0+cu111'

type(X), X.dtype --> (numpy.ndarray, dtype('float64'))

# Turn data into tensors
X = torch.from_numpy(X).type(torch.float)
y = torch.from_numpy(y).type(torch.float)

X[:5], y[:5]
     
(tensor([[ 0.7542,  0.2315],
         [-0.7562,  0.1533],
         [-0.8154,  0.1733],
         [-0.3937,  0.6929],
         [ 0.4422, -0.8967]]), tensor([1., 1., 1., 1., 0.]))



type(X), X.dtype, y.dtype --> (torch.Tensor, torch.float32, torch.float32)

# Split data into training and test sets

from sklearn.model_selection import train_test_split   --> train_test_split splits arrays or matrices into random train and test subsets.

X_train, X_test, y_train, y_test = train_test_split(X, --> X değişkenini buraya yazdık. Soldaki değişkenler bu sırayla yazılmalılar --> X_train, X_test, y_train, y_test                             			           
                                                    y, --> y değişkenini buraya yazdık.
                                                    test_size=0.2, # 0.2 = 20% of data will be test & 80% will be train
                                                    random_state=42) --> random seed ile aynı görevi yapıyor.
     

len(X_train), len(X_test), len(y_train), len(y_test)
     
(800, 200, 800, 200)

n_samples
     
1000

--------------------------------------------------------

Building a model
Let's build a model to classify our blue and red dots.

To do so, we want to:

Setup device agonistic code so our code will run on an accelerator (GPU) if there is one
Construct a model (by subclassing nn.Module)
Define a loss function and optimizer
Create a training and test loop

# Import PyTorch and nn
import torch
from torch import nn

# Make device agnostic code
device = "cuda" if torch.cuda.is_available() else "cpu"
device                                                 --> cpu
     


X_train

tensor([[ 0.6579, -0.4651],
        [ 0.6319, -0.7347],
        [-1.0086, -0.1240],
        ...,
        [ 0.0157, -1.0300],
        [ 1.0110,  0.1680],
        [ 0.5578, -0.5709]])


--------------------------------------------------------

Now we've setup device agnostic code, let's create a model that:

Subclasses nn.Module (almost all models in PyTorch subclass nn.Module)
Create 2 nn.Linear() layers that are capable of handling the shapes of our data
Defines a forward() method that outlines the forward pass (or forward computation) of the model
Instatiate an instance of our model class and send it to the target device

X_train.shape
     
torch.Size([800, 2])

y_train[:5]
     
tensor([1., 0., 0., 0., 1.])

from sklearn import datasets
# 1. Construct a model that subclasses nn.Module
class CircleModelV0(nn.Module):
  def __init__(self):
    super().__init__()
    # 2. Create 2 nn.Linear layers capable of handling the shapes of our data                                 --> 2 tane nn.Linear koyunca multi layer neural network oluşturmuş olduk.
    self.layer_1 = nn.Linear(in_features=2, out_features=5) # takes in 2 features and upscales to 5 features  --> 5 sayısı diğer nn.Linear'e gider ve yüksek değer vermek modelin kalitesini yükseltir ama yavaşlık sorunu oluşur.
    self.layer_2 = nn.Linear(in_features=5, out_features=1) # takes in 5 features from previous layer and outputs a single feature (same shape as y)
                                                                                                              Buradaki 2 değeri input layerindeki 2 neuron'i temsil ediyor.
                                                                                                                       5 değeri hidden layerdeki 5 neuron'i temsil ediyor.
														       1 değeri output layerindeki outputu temsil ediyor.
                                                                                                                     

  # 3. Define a forward() method that outlines the forward pass
  def forward(self, x):
    return self.layer_2(self.layer_1(x)) # x -> layer_1 ->  layer_2 -> output 
                                        layer 1'de linear regression formülü uygulanıyor output layer 2'ye gidiyor. layer 2'de linear regression formülü uygulanıyor.
                                        output return ediliyor.
                                                                                 

# 4. Instantiate an instance of our model class and send it to the target device
model_0 = CircleModelV0().to(device)
model_0
     
CircleModelV0(
  (layer_1): Linear(in_features=2, out_features=5, bias=True)
  (layer_2): Linear(in_features=5, out_features=1, bias=True)
)

device
     
cpu

next(model_0.parameters()).device
     
device(type='cpu')


-----


# Let's replicate the model above using nn.Sequential() --> sequential ile yukarıdaki kurduğumuz modeli kurabiliriz. Ama nn.module'i extend ettiğimiz yukarıdaki modelde 
							    forward methodunda istediğimizi yapabiliriz. Sequential sadece yukarı modeldeki gibi sıralamaya izin verir.
model_0 = nn.Sequential(
    nn.Linear(in_features=2, out_features=5),
    nn.Linear(in_features=5, out_features=1)
).to(device)

model_0
     
Sequential(
  (0): Linear(in_features=2, out_features=5, bias=True)
  (1): Linear(in_features=5, out_features=1, bias=True)
)

model_0.state_dict()
     
OrderedDict([('0.weight', tensor([[-0.1962,  0.3652],
                      [ 0.2764,  0.2147],
                      [ 0.5723, -0.5955],
                      [-0.2329,  0.0170],
                      [ 0.6259,  0.6916]], device='cuda:0')),
             ('0.bias',
              tensor([ 0.4193,  0.6624,  0.2594, -0.1640, -0.0477], device='cuda:0')),
             ('1.weight',
              tensor([[ 0.4129,  0.2358,  0.4115,  0.4045, -0.0853]], device='cuda:0')),
             ('1.bias', tensor([-0.4294], device='cuda:0'))])


# Make predictions
with torch.inference_mode():
  untrained_preds = model_0(X_test.to(device))

print(f"Length of predictions: {len(untrained_preds)}, Shape: {untrained_preds.shape}")
print(f"Length of test samples: {len(X_test)}, Shape: {X_test.shape}")
print(f"\nFirst 10 predictions:\n{torch.round(untrained_preds[:10])}") --> sonuçları yuvarladık.
print(f"\nFirst 10 labels:\n{y_test[:10]}")
     
Length of predictions: 200, Shape: torch.Size([200, 1])
Length of test samples: 200, Shape: torch.Size([200, 2]) --> len soldaki değeri verir. Buranın lengthi 200. 

First 10 predictions:
tensor([[-0.],
        [-0.],
        [-0.],
        [-0.],
        [0.],
        [0.],
        [-0.],
        [-0.],
        [-0.],
        [-0.]], device='cuda:0')

First 10 labels:
tensor([1., 0., 1., 0., 1., 1., 0., 0., 1., 0.])



X_test[:10], y_test[:10]
     
(tensor([[-0.3752,  0.6827],
         [ 0.0154,  0.9600],
         [-0.7028, -0.3147],
         [-0.2853,  0.9664],
         [ 0.4024, -0.7438],
         [ 0.6323, -0.5711],
         [ 0.8561,  0.5499],
         [ 1.0034,  0.1903],
         [-0.7489, -0.2951],
         [ 0.0538,  0.9739]]),
 tensor([1., 0., 1., 0., 1., 1., 0., 0., 1., 0.]))


--------------------------------------------------------

Setup loss function and optimizer
Which loss function or optimizer should you use?

Again... this is problem specific.

For example for regression you might want MAE or MSE (mean absolute error or mean squared error).

For classification you might want binary cross entropy or categorical cross entropy (cross entropy).

As a reminder, the loss function measures how wrong your models predictions are.

And for optimizers, two of the most common and useful are SGD and Adam, however PyTorch has many built-in options.

For some common choices of loss functions and optimizers - https://www.learnpytorch.io/02_pytorch_classification/#21-setup-loss-function-and-optimizer
For the loss function we're going to use torch.nn.BECWithLogitsLoss(), for more on what binary cross entropy (BCE) is, check out this article - https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a
For a defintion on what a logit is in deep learning - https://stackoverflow.com/a/52111173/7900723
For different optimizers see torch.optim



Binary cross-entropy is a loss function used in binary classification problems where the target variable has two possible outcomes, 0 and 1 and it measures the performance 
of the classification model whose output is a probability is a value between them. The goal of the model is to minimize this loss function during training to improve its 
predictive accuracy.

Kaynaklar=https://www.geeksforgeeks.org/binary-cross-entropy-log-loss-for-binary-classification/
Kaynaklar=https://en.wikipedia.org/wiki/Cross-entropy

Sigmoid Function aktivasyon fonksiyon'unudur. doğrusal olmayan(non-linear) fonksiyondur. Karar vermeye yönelik olasılıksal bir yaklaşımdır ve değer aralığı [0,1] arasındadır. 
Yani çıktının hangi sınıfa ait olduğuna dair olasılıksal bir değer verir bize. Sigmoid fonksiyonu sürekli yani türevlenebilir bir fonksiyon olduğundan öğrenme işlemi gerçekleşir.
Fakat Sigmoid mükemmel değildir. Çünkü türev değeri uç noktalarda sıfıra yakınsar (Vanishing Gradient).Bu durum Back Propagation sırasında öğrenmenin durmasına sebebiyet verir.

Kaynaklar=https://medium.com/databulls/yapay-sinir-a%C4%9Flar%C4%B1nda-aktivasyon-fonksiyonlar%C4%B1-11002b8ac522
Kaynaklar=https://en.wikipedia.org/wiki/Sigmoid_function


# Setup the loss function
# loss_fn = nn.BCELoss() # BCELoss = requires inputs to have gone through the sigmoid activation function prior to input to BCELoss
loss_fn = nn.BCEWithLogitsLoss() # BCEWithLogitsLoss = sigmoid activation function built-in

optimizer = torch.optim.SGD(params=model_0.parameters(),
                            lr=0.1)
     

# Calculate accuracy - out of 100 examples, what percentage does our model get right? 
def accuracy_fn(y_true, y_pred):
  correct = torch.eq(y_true, y_pred).sum().item() --> torch.eq iki tensordeki değerlerin eşitliğini kontrol ediyor. Eşit oldukları noktalarda true yazıyor. sum, true değerleriyle de 
  acc = (correct/len(y_pred)) * 100                   kullanılabiliyor ve true değerlerini topluyor. Yani 5 true değeri varsa tensor(5) yazıyor. item yazarak da integer alıyoruz.
  return acc

-----

Train model

To train our model, we're going to need to build a training loop with the following steps:

Forward pass
Calculate the loss
Optimizer zero grad
Loss backward (backpropagation)
Optimizer step (gradient descent)


Going from raw logits -> prediction probabilities -> prediction labels

Our model outputs are going to be raw logits. --> logit buna deniyor. 

We can convert these logits into prediction probabilities by passing them to some kind of activation function (e.g. sigmoid for binary classification and 
softmax for multiclass classification).

Then we can convert our model's prediction probabilities to prediction labels by either rounding them or taking the argmax().


# View the first 5 outputs of the forward pass on the test data
model_0.eval() 
with torch.inference_mode():
  y_logits = model_0(X_test.to(device))[:5]
y_logits
     
tensor([[-0.1481],                    --> Burası logit
        [-0.1465],
        [-0.0762],
        [-0.1688],
        [ 0.0445]], device='cuda:0')

y_test[:5]
     
tensor([1., 0., 1., 0., 1.])

# Use the sigmoid activation function on our model logits to turn them into prediction probabilities
y_pred_probs = torch.sigmoid(y_logits) 
y_pred_probs
     
tensor([[0.4630],
        [0.4634],
        [0.4810],
        [0.4579],
        [0.5111]], device='cuda:0')

For our prediction probability values, we need to perform a range-style rounding on them:

y_pred_probs >= 0.5, y=1 (class 1)
y_pred_probs < 0.5, y=0 (class 0)

# Find the predicted labels 
y_preds = torch.round(y_pred_probs) --> round kullandık.

# In full (logits -> pred probs -> pred labels)
y_pred_labels = torch.round(torch.sigmoid(model_0(X_test.to(device))[:5]))

# Check for equality
print(torch.eq(y_preds.squeeze(), y_pred_labels.squeeze()))

# Get rid of extra dimension
y_preds.squeeze()              
     
tensor([True, True, True, True, True], device='cuda:0')
tensor([0., 0., 0., 0., 1.], device='cuda:0')

y_test[:5]
     
tensor([1., 0., 1., 0., 1.])

-----

Building a training and testing loop

torch.manual_seed(42)
torch.cuda.manual_seed(42)  --> Set the seed for generating random numbers for the current GPU.  

# Set the number of epochs
epochs = 100

# Put data to target device 
X_train, y_train = X_train.to(device), y_train.to(device)
X_test, y_test = X_test.to(device), y_test.to(device)

# Build training and evaluation loop
for epoch in range(epochs):
  ### Training
  model_0.train()

  # 1. Forward pass
  y_logits = model_0(X_train).squeeze()
  y_pred = torch.round(torch.sigmoid(y_logits)) # turn logits -> pred probs -> pred labels

  # 2. Calculate loss/accuracy
  # loss = loss_fn(torch.sigmoid(y_logits), # nn.BCELoss expects prediction probabilities as input
  #                y_train)
  loss = loss_fn(y_logits, # nn.BCEWithLogitsLoss expects raw logits as input
                 y_train)
  acc = accuracy_fn(y_true=y_train, --> Bu kısma gerek yok ama model daha detaylı dursun diye ekledik.
                    y_pred=y_pred)
  
  # 3. Optimizer zero grad
  optimizer.zero_grad()

  # 4. Loss backward (backpropagation)
  loss.backward()

  # 5. Optimizer step (gradient descent)
  optimizer.step() 

  ### Testing
  model_0.eval()
  with torch.inference_mode():
    # 1. Forward pass 
    test_logits = model_0(X_test).squeeze()
    test_pred = torch.round(torch.sigmoid(test_logits))

    # 2. Calculate test loss/acc
    test_loss = loss_fn(test_logits,
                        y_test)
    test_acc = accuracy_fn(y_true=y_test,
                           y_pred=test_pred)
  
  # Print out what's happenin'
  if epoch % 10 == 0:
    print(f"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%") --> .2f .5f ler noktadan sonra kaç basamak yazılacağını gösteriyor.
     
Epoch: 0 | Loss: 0.69461, Acc: 47.88% | Test loss: 0.69301, Test acc: 48.50%
Epoch: 10 | Loss: 0.69402, Acc: 48.75% | Test loss: 0.69296, Test acc: 48.00%
Epoch: 20 | Loss: 0.69369, Acc: 49.50% | Test loss: 0.69308, Test acc: 46.50%
Epoch: 30 | Loss: 0.69348, Acc: 50.62% | Test loss: 0.69325, Test acc: 45.50%
Epoch: 40 | Loss: 0.69334, Acc: 50.12% | Test loss: 0.69341, Test acc: 48.50%
Epoch: 50 | Loss: 0.69324, Acc: 49.25% | Test loss: 0.69357, Test acc: 51.00%
Epoch: 60 | Loss: 0.69317, Acc: 49.50% | Test loss: 0.69372, Test acc: 50.50%
Epoch: 70 | Loss: 0.69312, Acc: 50.38% | Test loss: 0.69385, Test acc: 49.00%
Epoch: 80 | Loss: 0.69308, Acc: 50.12% | Test loss: 0.69396, Test acc: 49.50%
Epoch: 90 | Loss: 0.69305, Acc: 50.50% | Test loss: 0.69406, Test acc: 48.00%


Make predictions and evaluate the model


From the metrics it looks like our model isn't learning anything...

So to inspect it let's make some predictions and make them visual!

To do so, we're going to import a function called plot_decision_boundary() - https://github.com/mrdbourke/pytorch-deep-learning/blob/main/helper_functions.py --> Forkladığımız projede var.


import requests
from pathlib import Path

# Download helper functions from Learn PyTorch repo (if it's not already downloaded)
if Path("helper_functions.py").is_file():
  print("helper_functions.py already exists, skipping download")
else:
  print("Downloading helper_functions.py")
  request = requests.get("https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py") --> ilgili yerde raw olanı kopyaladık.
															   
  with open("helper_functions.py", "wb") as f:                          --> yeni dosya oluşturduk.
    f.write(request.content)                                            --> dosyanın içine grafiklerle ilgili bir sürü function yazıldı.

from helper_functions import plot_predictions, plot_decision_boundary
     
Downloading helper_functions.py



# Plot decision boundary of the model
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)       --> grafikler buradaki row column ve indexe göre sıraya giriyor.
plt.title("Train")
plot_decision_boundary(model_0, X_train, y_train) --> train verilerini modelde gösterdik. 4.1'deki grafik oluştu. Grafik yukarıdaki verilerin grafiği değil.
plt.subplot(1, 2, 2)                                  
plt.title("Test")
plot_decision_boundary(model_0, X_test, y_test) 




--------------------------------------------------------
