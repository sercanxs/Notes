Neural Network

https://en.wikipedia.org/wiki/Neural_network_(machine_learning)

---------------------------------------------
Tensor

https://en.wikipedia.org/wiki/Tensor


----


https://colab.research.google.com/  = python codeleri burada çalıştırılabilir. Runtime kısmında gpu tpu seçilebilir. Gpu seçtik. (Kota doldurmasın diye gpu'yu gerektiğinde kullanıyoruz)

!nvidia-smi = kullanılan gpu'yu verdi. Google'in ücretsiz gpu'su --> Tesla T4

-----




import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
print(torch.__version__)         = pytorch versiyonunu yazdı --> 2.4.1+cu121


scalar = torch.tensor(7)  --> tensor oluşturduk.

scalar                        --> tensor(7)
scalar.item() = integer yazdı --> 7 


scalar.ndim               --> Dim, dimensions'i temsil ediyor. Boyut sayısını gösteriyor --> 0 




Google colab kodları birkaç saat sonra resetliyor. Tekrar çalıştırmak gerekebilir.


vector = torch.tensor([7,7])
vector                        --> tensor([7, 7])
vector.ndim                   --> 1
vector.shape                  --> torch.Size([2]) yazdı. Yani vektörde 2 tane element var.



MATRIX = torch.tensor([[7,8],
                      [9,10]])
MATRIX                          --> tensor([[ 7,  8],
                                           [ 9, 10]])


MATRIX.ndim                     --> 2


MATRIX[0] = Matrix'in birinci sırasında bulunan değeri yazdı. --> tensor([7, 8]) 


MATRIX.shape = torch.Size([2, 2]) yazdı. yani matrixin şekli 2x2 yani matrixte 4 element var.




TENSOR = torch.tensor([[[1, 2, 3],
                        [3, 6, 9],
                        [2, 4, 5]]])
TENSOR                                  --> tensor([[[1, 2, 3],
                                                     [3, 6, 9],
                                                     [2, 4, 5]]])


TENSOR.ndim                             --> 3
TENSOR.shape                            --> torch.Size([1, 3, 3])

TENSOR[0]                               --> tensor([[1, 2, 3],
        			                    [3, 6, 9],
        					    [2, 4, 5]])



random_tensor=torch.rand(3,4)   --> random tensor oluşturduk.
random_tensor                   --> tensor([[0.0554, 0.0659, 0.9163, 0.6129],
        			            [0.9525, 0.8096, 0.5071, 0.3709],
        				    [0.6432, 0.1050, 0.6121, 0.5664]])


random_tensor=torch.rand(3,4)
random_tensor=torch.rand(size=(3,4))      = size yazılsa da yazılmasa da bir şey değişmez.


zeros = torch.zeros(3,4) = torch.zeros yazınca bütün değerler sıfır olarak geldi.
zeros


ones = torch.ones(3,4) = torch.ones ekleyince bütün değerler bir olarak gelir.
ones




torch.arange(1,11) --> arange yazınca 1'den 11'e kadar sayı yazdı. 11 dahil değil. range deprecated artık arange kullanılıyor. Sonuç --> tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])


torch.arange(start=0,end=1000,step=133) --> start ve end kısmı bu şekilde gösterilebilir. step ise sayıların kaçar kaçar artacağını gösterir. 
                                            Sonuç --> tensor([  0, 133, 266, 399, 532, 665, 798, 931])




one_to_ten = torch.arange(1,11)
one_to_ten                      --> tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])


ten_zeros = torch.zeros_like(input=one_to_ten) = torch.zeros_like yazınca belirli tensorle aynı shapede değerleri 0 olan yeni tensor oluşturduk. içeri input olarak değişken yazdık.
ten_zeros                       --> tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])


--------



float_32_tensor=torch.tensor([3.0, 6.0, 9.0],  = torch.float32 tensorler için default datatype'dir. tensor oluştururken dtype=None yazsak bile tensorun datatype'i torch.float32'dir.
                               dtype=None)
float_32_tensor                               --> tensor([3., 6., 9.])


float_32_tensor.dtype                         = dtype yazarak datatype'i alırız --> torch.float32 



float_16_tensor=torch.tensor([3.0, 6.0, 9.0],
                               dtype=torch.float16)  = data type'i değiştirdik.

float_16_tensor.dtype --> torch.float16


-----


float_32_tensor = torch.tensor([3.0, 6.0, 9.0],
                               dtype=None, # defaults to None, which is torch.float32 or whatever datatype is passed
                               device=None, # What device is your tensor on
                               requires_grad=False) # whether or not to track gradients with this tensors operations

float_32_tensor.shape, float_32_tensor.dtype, float_32_tensor.device  --> (torch.Size([3]), torch.float32, device(type='cpu'))


-----


float_16_tensor=float_32_tensor.type(torch.float16) = belirli tensorun datatype'ini bu şekilde değiştirdik. torch.float32 artık torch.float16 oldu.
float_16_tensor




float_16_tensor.dtype --> torch.float16


---------------------------------------------

some_tensor.size() = size ile shape aynıdır. ama size function ve shape ise attribute'dir.
some_tensor.shape


Getting information from tensors (tensor attributes)
Tensors not right datatype - to do get datatype from a tensor, can use tensor.dtype
Tensors not right shape - to get shape from a tensor, can use tensor.shape
Tensors not on the right device - to get device from a tensor, can use tensor.device

---------
Manipulating Tensors (tensor operations)

Addition
Subtraction
Multiplication (element-wise)
Division
Matrix multiplication



Addition

tensor = torch.tensor([1, 2, 3])
tensor + 10                       --> tensor([11, 12, 13])

Subtraction

tensor - 10                       --> tensor([-9, -8, -7])

Multiplication (element-wise)

tensor * 10                       --> tensor([10, 20, 30])


torch.add(tensor, 10) = toplama ve çarpma için bu functionlar da kullanılabilir.
torch.mul(tensor, 10)





Two main ways of performing multiplication in neural networks and deep learning:

Element-wise multiplication   = çarpanlardan birisi scalar ise element wise vardır.
Matrix mutliplication (dot product)


tensor = torch.tensor([1, 2, 3])
print(tensor, "*", tensor)
print(f"Equals: {tensor * tensor}")

sonuç =

tensor([1, 2, 3]) * tensor([1, 2, 3]) = ikisi vektörse bu elementlerin çarpımı element-wise multiplication'dır
Equals: tensor([1, 4, 9])

-----
torch.matmul(tensor, tensor) = tensor(14) --> iki vektörün çarpımında matrix multiplication kullanıldığında sonuç tensor([1, 4, 9])
                                              değil de tensor(14) çıkıyor.

torch.matmul(tensor, tensor) = işlemini for loop kullanmak yerine pytorch methodlarıyla yapınca süre daha kısa oluyor.

-----

torch.matmul(tensor, tensor) = torch.matmul matrix multiplication için kullanılır
tensor @ tensor              = yukarıdaki ile bu aynıdır. ikisi de matrix multiplication için kullanılabilir.

torch.mm ile torch.matmul aynıdır.



To fix our tensor shape issues, we can manipulate the shape of one of our tensors using a transpose.

A transpose switches the axes or dimensions of a given tensor.


tensor_B = torch.tensor([[7, 10],
                         [8, 11],
                         [9, 12]]) 


tensor_B     --> tensor([[ 7, 10],
                         [ 8, 11],
                         [ 9, 12]])


tensor_B.T   --> tensor([[ 7,  8,  9],  --> sonuna T ekleyince tensorun şekli değişti. matrixler birbiriyle çarpılamadığında iş yapabilir.
                        [10, 11, 12]])




Finding the min, max, mean, sum, etc (tensor aggregation)


x = torch.arange(0,100,10)
x                             --> tensor([ 0, 10, 20, 30, 40, 50, 60, 70, 80, 90])

x.dtype                       --> torch.int64 yazdı. yani arange kullanınca datatype'i torch.float32 değil de torch.int64 oldu.


torch.min(x) = tensor(0) --> minimum değeri verdi
x.min() = torch.min() yerine bu yazılabilir.

torch.max(x) = tensor(90) --> maximum değeri verdi


torch.mean(x.type(torch.float32)) = tensor(45.) --> ortalama değeri verdi. Ortalama değeri alırken her data type'i desteklemeyebilir. dolayısıyla tensorun datatype'ini değiştirdik.
x.mean() = torch.mean() yerine bu yazılabilir.



torch.sum(x) = tensor(450) --> bütün sayıların toplamını verdi.
x.sum() = torch.sum() yerine bu kullanılabilir.  



Finding the positional min and max


x = torch.arange(1,100,10)
x                         --> tensor([ 1, 11, 21, 31, 41, 51, 61, 71, 81, 91])


x.argmin() = tensor(0) --> En küçük değerin indexini verdi
x.argmax() = tensor(9) --> En büyük değerin indexini verdi







Reshaping, stacking, squeezing and unsqueezing tensors

Reshaping - reshapes an input tensor to a defined shape
View - Return a view of an input tensor of certain shape but keep the same memory as the original tensor
Stacking - combine multiple tensors on top of each other (vstack) or side by side (hstack)
Squeeze - removes all 1 dimensions from a tensor
Unsqueeze - add a 1 dimension to a target tensor
Permute - Return a view of the input with dimensions permuted (swapped) in a certain way


x = torch.arange(1., 10.)
x                             --> tensor([1., 2., 3., 4., 5., 6., 7., 8., 9.])

x.shape                       --> torch.Size([9])


x_reshaped = x.reshape(1, 9) = reshape kullanarak tensorun şeklini değiştirdik. Yalnız tensordeki değerlerin kaybolmaması için var olan boyutlara küçük değer girilemez.
x_reshaped                    --> tensor([[1., 2., 3., 4., 5., 6., 7., 8., 9.]]) --> yeni boyut eklendi.

x_reshaped.shape              --> torch.Size([1, 9])

x_reshaped = x.reshape(2, 9)  --> Boyutlar değiştirilirken oluşan yeni şekilde mevcut bulunan eleman sayısınca yer olmalıdır. Yani 9x1, 1x9 ve 3x3 yer olabilir ama 2x9 olursa toplam
x_reshaped                        yer sayısı 18 olur ve 9 yer boş kalır. Program hata verir.

x_reshaped = x.reshape(3, 3)
x_reshaped                    --> tensor([[1., 2., 3.],
       					  [4., 5., 6.],
       					  [7., 8., 9.]])


---------

z = x.view(1, 9)  --> view kullandık.
z                 --> tensor([[1., 2., 3., 4., 5., 6., 7., 8., 9.]])



# Changing z changes x (because a view of a tensor shares the same memory as the original input) --> view, reshape gibidir ama yeni tensör değiştiğinde eskisi de değişir.
z[:, 0] = 5                                                      --> z'nin 1'inci değerini değiştirdik. normalde z[0]=5 yazarsak bütün sırayı içereceğinden dolayı bütün sıra 5 olurdu.
                                                                     z[0][0]=5 yazarak tek bir sayıyı işaret edebiliriz. Burada ise : kullanılmış, numpy/pandas ile alakalı olabilir.
z            --> tensor([[5., 2., 3., 4., 5., 6., 7., 8., 9.]])  --> z'nin ilk değeri 5 oldu.



x  --> tensor([5., 2., 3., 4., 5., 6., 7., 8., 9.])              --> z değişince x de değişti.


---------

x = torch.arange(1., 10.)
x                         --> tensor([1., 2., 3., 4., 5., 6., 7., 8., 9.])


# Stack tensors on top of each other
x_stacked = torch.stack([x, x, x, x], dim=0)         --> stack ekleyince tensorler üst üste eklendi.


x_stacked  --> tensor([[1., 2., 3., 4., 5., 6., 7., 8., 9.],
       		       [1., 2., 3., 4., 5., 6., 7., 8., 9.],
        	       [1., 2., 3., 4., 5., 6., 7., 8., 9.],
        	       [1., 2., 3., 4., 5., 6., 7., 8., 9.]])



x_stacked = torch.stack([x, x, x, x], dim=1)          --> dim'i bir yapınca değerler alt alta sıralandı. dim'i 2 yapınca hata verdi çünkü stacklenmek için alınan tensor dimensionun
							  2 olması için uygun değil.


x_stacked  --> tensor([[1., 1., 1., 1.],
        	       [2., 2., 2., 2.],
     		       [3., 3., 3., 3.],
       		       [4., 4., 4., 4.],
       		       [5., 5., 5., 5.],
       		       [6., 6., 6., 6.],
       		       [7., 7., 7., 7.],
       		       [8., 8., 8., 8.],
      		       [9., 9., 9., 9.]])


---------------------------------------------

# torch.squeeze() - removes all single dimensions from a target tensor


random_tensor=torch.rand(1,3,1,4)
random_tensor                        --> tensor([[[[0.6112, 0.0567, 0.5847, 0.5393]],

         					  [[0.8180, 0.3211, 0.3559, 0.8263]],

         					  [[0.5220, 0.9734, 0.9233, 0.6857]]]])



random_tensor.shape  --> torch.Size([1, 3, 1, 4])


x_squeezed = random_tensor.squeeze() --> squeeze kullanınca 1 değer içeren boyutlar silindi.
x_squeezed                           --> tensor([[0.6112, 0.0567, 0.5847, 0.5393],
      					         [0.8180, 0.3211, 0.3559, 0.8263],
     					         [0.5220, 0.9734, 0.9233, 0.6857]])


x_squeezed.shape  --> torch.Size([3, 4]) --> önceden var olan 1'ler yok oldu.


# torch.unsqueeze() - adds a single dimension to a target tensor at a specific dim (dimension)



x_unsqueezed = x_squeezed.unsqueeze(dim=0) --> unsqueeze ekleyince belirli sıraya yeni boyut eklendi.
x_unsqueezed                               --> tensor([[[0.0382, 0.1392, 0.6659, 0.4221],
         						[0.5023, 0.8650, 0.0075, 0.4094],
      						        [0.1181, 0.5211, 0.0024, 0.3381]]])


x_unsqueezed.shape --> torch.Size([1, 3, 4]) --> buraya 1 eklendi.


# torch.permute - rearranges the dimensions of a target tensor in a specified order


random_tensor=torch.rand(224,50,75,9)
random_tensor.shape                  --> torch.Size([224, 50, 75, 9])


x_permuted = random_tensor.permute(3, 1, 2, 0) --> permute ekleyince önceki şekildeki boyutların indexlerini değiştirdik.
x_permuted.shape                     --> torch.Size([9, 50, 75, 224])




random_tensor[0, 0, 0, 0] = 728218
random_tensor[0, 0, 0, 0]           --> tensor(728218.) --> permute, view olduğu için önceki tensor değiştiğinde yeni tensor de değişir.
                                                            Değerler her ne kadar aynı olsa da şekilleri farklıdır. Buradaki asıl sorun
                                                            eğer şekiller farklı ise önceki tensorde değiştirilen yer yeni tensordeki aynı
                                                            yere mi denk geliyor.

x_permuted[0, 0, 0, 0]              --> tensor(728218.) --> şekiller farklı olmasına rağmen bu şekilde index girince değişikliği
                                                            görebildik. index konusuna daha gelmedik.



----------

Indexing (selecting data from tensors)


x = torch.arange(1, 10).reshape(1, 3, 3)
x                                        --> tensor([[[1, 2, 3],
      				                      [4, 5, 6],
     		 		                      [7, 8, 9]]])

x.shape --> torch.Size([1, 3, 3])



x[0] --> tensor([[1, 2, 3],   --> index verdik.
                 [4, 5, 6],
                 [7, 8, 9]])


x[0][0] --> tensor([1, 2, 3])
x[0, 0] = yukarıdaki yerine bu kullanılabilir. ikisi de aynı sonucu verir.


x[0][0][0] --> tensor(1)


x[:,0] --> tensor([[1, 2, 3]]) --> önce bir boyuttaki bütün elementleri aldık. Sonra o elementlerin 1. değerini aldık.


x[:, :, 1] --> tensor([[2, 5, 8]]) --> önce bir boyuttaki bütün elementlari aldık. Normalde index verip bir tane seçebiliyorduk ama bu
                                       şekilde o boyutta olan hepsini seçebildik. sonra bir alt boyuttaki var olan bütün elemanları 
                                       aldık. dolayısıyla elimizde ilk iki noktadan sonra bir element vardı. ikinci iki noktadan sonra
                                       3 element oldu. o 3 elementin ise 2. değerlerini seçtik. Yani [1, 2, 3]'te 2'yi [4, 5, 6]'da 5'i
				       [7, 8, 9]'da 8'i seçtik. tensörde bütün elementler bulunmasına rağmen sadece bir tanesini seçiyorduk.
				       Bu şekilde birden fazla tensör değeri seçebildik.



x[0][0][0] --> tensor(1)
x[:,0,0]   --> tensor([1]) --> birden fazla değer içerebileceği için bunda square bracket var.

-----

PyTorch tensors & NumPy

Since NumPy is a popular Python numerical computing library, PyTorch has functionality to interact with it nicely.

The two main methods you'll want to use for NumPy to PyTorch (and back again) are:

torch.from_numpy(ndarray) - NumPy array -> PyTorch tensor.
torch.Tensor.numpy() - PyTorch tensor -> NumPy array.




array = np.arange(1.0, 8.0)       --> numpy array oluşturduk.
tensor = torch.from_numpy(array)  --> tensore çevirdik. # warning: when converting from numpy -> pytorch, pytorch reflects numpy's default datatype of float64 unless specified otherwise

array                             --> array([1., 2., 3., 4., 5., 6., 7.])
tensor                            --> tensor([1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64)  




tensor = torch.ones(7)
numpy_tensor = tensor.numpy()  --> Tensor to NumPy array



tensor          --> tensor([1., 1., 1., 1., 1., 1., 1.])


numpy_tensor    --> array([1., 1., 1., 1., 1., 1., 1.], dtype=float32)


--------

Reproducbility (trying to take random out of random)

To reduce the randomness in neural networks and PyTorch comes the concept of a random seed.

Essentially what the random seed does is "flavour" the randomness.

RANDOM_SEED = 42                    = istediğimiz numarayı yazdık.
torch.manual_seed(RANDOM_SEED)      = her random tensorden önce torchlara seed verince tensorde rastgele oluşan değerler aynı oluyor. Bu da reproducibility'i artırıyor. ama seedler aynı 
random_tensor_C = torch.rand(3, 4)    olsa bile çıkan sonuçların device, bilgisayar vs. aynı olmadığında farklı olabilme riski var.

torch.manual_seed(RANDOM_SEED)
random_tensor_D = torch.rand(3, 4)

print(random_tensor_C)
print(random_tensor_D)
print(random_tensor_C == random_tensor_D)




Extra resources for reproducibility:

https://pytorch.org/docs/stable/notes/randomness.html
https://en.wikipedia.org/wiki/Random_seed


---------------------------------------------







