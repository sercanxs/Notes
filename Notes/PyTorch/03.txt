import torch
from torch import nn            --> nn contains all of PyTorch's building blocks for neural networks 
import matplotlib.pyplot as plt

# Check PyTorch version
torch.__version__

----

1. Data (preparing and loading)
Data can be almost anything... in machine learning.

Excel speadsheet
Images of any kind
Videos (YouTube has lots of data...)
Audio like songs or podcasts
DNA
Text

Machine learning is a game of two parts:

Get data into a numerical representation.
Build a model to learn patterns in that numerical representation.


Linear regression

In statistics, linear regression is a statistical model that estimates the linear relationship between a scalar response (dependent variable) and 
one or more explanatory variables (regressor or independent variable). The case of one explanatory variable is called simple linear regression; 
for more than one, the process is called multiple linear regression.[1] This term is distinct from multivariate linear regression, 
where multiple correlated dependent variables are predicted, rather than a single scalar variable.[2] If the explanatory variables are measured 
with error then errors-in-variables models are required, also known as measurement error models.


Doğrusal regresyonda, karmaşık sayı setini bir doğru formülüne indirgemeye çalışılır. Buna göre tüm noktalara en yakından geçen bir doğru çizmek amaçlanır.
Bu hata miktarının en az tutulmasıdır. Hata, verilerdeki yanlışlık değil, sapmadır. Yanlış olan modeldir.
Doğrusal regresyon ile amaçlanan bu noktaların tamamına en yakın geçen doğruyu elde etmektir. Her doğrunun bir formülü olduğu gibi bu doğrunun da karakteristik
bir şekilde y = ax + b denklemine uygun bir formülü, bir (a,b) ikilisi bulunacaktır.

Kaynaklar = https://en.wikipedia.org/wiki/Linear_regression
	    https://aydemirhamza.medium.com/do%C4%9Frusal-lineer-regresyon-de873548e0fd





To showcase this, let's create some known data using the linear regression formula.
We'll use a linear regression formula to make a straight line with known parameters.


# Create *known* parameters
weight = 0.7
bias = 0.3

# Create
start = 0
end = 1
step = 0.02
X = torch.arange(start, end, step).unsqueeze(dim=1) # without unsqueeze, errors will pop up --> ileride sorun olabilir diye unsqueeze uyguladık.
y = weight * X + bias                               --> y = bx + a  --> bu denklemde a ve b parametreler olabilir.

X[:10], y[:10]       --> 10. indexin solundakileri yazdı 10. index dahil değil.

Sonuç

(tensor([[0.0000],
         [0.0200],
         [0.0400],
         [0.0600],
         [0.0800],
         [0.1000],
         [0.1200],
         [0.1400],
         [0.1600],
         [0.1800]]),
 tensor([[0.3000],
         [0.3140],
         [0.3280],
         [0.3420],
         [0.3560],
         [0.3700],
         [0.3840],
         [0.3980],
         [0.4120],
         [0.4260]]))


len(X), len(y)  --> (50, 50)


-------------------------------------------------------------------

Splitting data into training and test sets (one of the most important concepts in machine learning in general)
Let's create a training and test set with our data.


# Create a train/test split
train_split = int(0.8 * len(X))
X_train, y_train = X[:train_split], y[:train_split]
X_test, y_test = X[train_split:], y[train_split:] 

len(X_train), len(y_train), len(X_test), len(y_test) --> (40, 40, 10, 10)


-----

How might we better visualize our data?


def plot_predictions(train_data=X_train,          = function oluşturduk.
                     train_labels=y_train,
                     test_data=X_test,
                     test_labels=y_test,
                     predictions=None):
  """
  Plots training data, test data and compares predictions.
  """
  plt.figure(figsize=(10, 7))  = grafiğin boyutunu ayarladık. Matplotlib'in özelliğini kullandık.

  # Plot training data in blue
  plt.scatter(train_data, train_labels, c="b", s=4, label="Training data") --> c, colour'i temsil ediyor. b ise blue'yi temsil ediyor. s, ise grafikte nokta olarak gösterilen verilerin 
									       boyutunu ayarlıyor. Bunun yanında ekstradan label da verdik.

  # Plot test data in green
  plt.scatter(test_data, test_labels, c="g", s=4, label="Testing data") --> train data'si yanında test datasını da ayarladık. colour'i ise g yani green yaptık.

  # Are there predictions?
  if predictions is not None:             = tahminimiz olmadığı için predictions değişkenini None olarak ayarlamıştık. eğer none olmasaydı predictions'i da grafikte gösterecektik.
    # Plot the predictions if they exist
    plt.scatter(test_data, predictions, c="r", s=4, label="Predictions")
  
  # Show the legend
  plt.legend(prop={"size": 14}); = yukarıda verilere label girmiştik ama bu label grafikte gösterilmiyordu. legend oluşturunca labeller, grafikte gösteriliyor. size label'in
                                   boyutunu belirtiyor. 
     

plot_predictions(); --> functionı çağırınca ekranda grafik oluştu.



-------------------------------------------------------------------

Our first PyTorch model!



Because we're going to be building classes throughout the course, I'd recommend getting familiar with OOP in Python, to do so 
you can use the following resource from Real Python: https://realpython.com/python3-object-oriented-programming/

What our model does:

Start with random values (weight & bias)
Look at training data and adjust the random values to better represent (or get closer to) the ideal 
values (the weight & bias values we used to create the data) 
How does it do so?

Through two main algorithms: PyTorch bunları arkada otomatik yapıyor.

Gradient descent - https://youtu.be/IHZwWFHWa-w
Backpropagation - https://youtu.be/Ilg3gGewQ5U

Gradient Descent, rastgele alınan değişkenlerle başlayarak global minimum değerine ulaşmayı amaçlar. Hesaplamalarında türevler var.


Backpropagation is a machine learning technique essential to the optimization of artificial neural networks. It facilitates the use
of gradient descent algorithms to update network weights, which is how the deep learning models driving modern artificial intelligence (AI) “learn.”
Abstractly speaking, the purpose of backpropagation is to train a neural network to make better predictions through supervised learning.
More fundamentally, the goal of backpropagation is to determine how model weights and biases should be adjusted to minimize error as measured by a "loss function".




from torch import nn --> nn'i import ettik.

# Create linear regression model class
class LinearRegressionModel(nn.Module): # <- almost everything in PyTorch inherhits from nn.Module
  def __init__(self):                   --> constructor
    super().__init__()
    self.weights = nn.Parameter(torch.randn(1, # <- start with a random weight and try to adjust it to the ideal weight    --> parametre oluşturduk.
                                            requires_grad=True, # <- can this parameter be updated via gradient descent? default=True.     
                                            dtype=torch.float)) # <- PyTorch loves the datatype torch.float32 (datatypelerin birden çok ismi olabilir.)
    
    self.bias = nn.Parameter(torch.randn(1, # <- start with a random bias and try to adjust it to the ideal bias
                                         requires_grad=True, # <- can this parameter be updated via gradient descent? 
                                         dtype=torch.float)) # <- PyTorch loves the datatype torch.float32 
    
  # Forward method to define the computation in the model
  def forward(self, x: torch.Tensor) -> torch.Tensor: # <- "x" is the input data --> x'in torch.Tensor olması gerekir.
    return self.weights * x + self.bias # this is the linear regression formula



x:torch.Tensor  =  means that the function expects x to be of type tensor.

-> torch.tensor =  means that the functions should return a value of type tensor.

-----


PyTorch model building essentials

torch.nn - contains all of the buildings for computational graphs (a neural network can be considered a computational graph)
torch.nn.Parameter - what parameters should our model try and learn, often a PyTorch layer from torch.nn will set these for us
torch.nn.Module - The base class for all neural network modules, if you subclass it, you should overwrite forward()
torch.optim - this where the optimizers in PyTorch live, they will help with gradient descent
def forward() - All nn.Module subclasses require you to overwrite forward(), this method defines what happens in the forward computation


See more of these essential modules via the PyTorch cheatsheet - https://pytorch.org/tutorials/beginner/ptcheat.html

-----

Checking the contents of our PyTorch model
Now we've created a model, let's see what's inside...

So we can check our model parameters or what's inside our model using .parameters().


# Create a random seed
torch.manual_seed(42)

# Create an instance of the model (this is a subclass of nn.Module)
model_0 = LinearRegressionModel()

# Check out the parameters
list(model_0.parameters())        --> modeldeki parametrelere bakmak için list içinde yazdık. 
     
[Parameter containing:
 tensor([0.3367], requires_grad=True),
 Parameter containing:
 tensor([0.1288], requires_grad=True)]

# List named parameters
model_0.state_dict()              --> bu şekilde yazınca parametreler isimleriyle birlikte geliyor.
     
OrderedDict([('weights', tensor([0.3367])), ('bias', tensor([0.1288]))])

-----



Making prediction using torch.inference_mode()


To check our model's predictive power, let's see how well it predicts y_test based on X_test.

When we pass data through our model, it's going to run it through the forward() method.


y_preds = model_0(X_test) --> modelimize verilerimizi ekledik. model verilerimizi çalıştırdı ve x'e göre y'i tahmin etti.
y_preds
     
tensor([[0.3982],
        [0.4049],
        [0.4116],
        [0.4184],
        [0.4251],
        [0.4318],
        [0.4386],
        [0.4453],
        [0.4520],
        [0.4588]], grad_fn=<AddBackward0>)

# Make predictions with model
with torch.inference_mode(): --> inference mode koymayınca yukarıdaki gibi grad_fn oluşuyor. inference mode bazı özellikleri kapatıyor ve model daha hızlı çalışıyor.
  y_preds = model_0(X_test)
  

# # You can also do something similar with torch.no_grad(), however, torch.inference_mode() is preferred
# with torch.no_grad():
#   y_preds = model_0(X_test)

y_preds
     
tensor([[0.3982],
        [0.4049],
        [0.4116],
        [0.4184],
        [0.4251],
        [0.4318],
        [0.4386],
        [0.4453],
        [0.4520],
        [0.4588]]) --> inference mode'de grad_fn oluşmadı.


See more on inference mode here - https://twitter.com/PyTorch/status/1437838231505096708?s=20&t=cnKavO9iTgwQ-rfri6u7PQ


y_test
     
tensor([[0.8600],
        [0.8740],
        [0.8880],
        [0.9020],
        [0.9160],
        [0.9300],
        [0.9440],
        [0.9580],
        [0.9720],
        [0.9860]]) --> önceden yaptığımız, parametreleri doğru ayarladığımız modelin tahmini (Belki bu model değildir.)


plot_predictions(predictions=y_preds) --> predictions'in default value'si None'du. Ona modelin yaptığı tahmini ekledik. grafikte gösterdik.


-------------------------------------------------------------------

Train model


The whole idea of training is for a model to move from some unknown parameters (these may be random) to some known parameters.

Or in other words from a poor representation of the data to a better representation of the data.

One way to measure how poor or how wrong your models predictions are is to use a loss function.

Note: Loss function may also be called cost function or criterion in different areas. For our case, we're going to refer to it as a loss function.


Things we need to train:

Loss function: A function to measure how wrong your model's predictions are to the ideal outputs, lower is better.
Optimizer: Takes into account the loss of a model and adjusts the model's parameters (e.g. weight & bias in our case) to improve the loss function - https://pytorch.org/docs/stable/optim.html#module-torch.optim
         Inside the optimizer you'll often have to set two parameters:
                       params - the model parameters you'd like to optimize, for example params=model_0.parameters()
                       lr (learning rate) - the learning rate is a hyperparameter that defines how big/small the optimizer changes the parameters with each step (a small lr results in small changes, a large lr results in large changes)

And specifically for PyTorch, we need:

A training loop
A testing loop

list(model_0.parameters())
     
[Parameter containing:
 tensor([0.3367], requires_grad=True),
 Parameter containing:
 tensor([0.1288], requires_grad=True)]

# Check out our model's parameters (a parameter is a value that the model sets itself)
model_0.state_dict()
     
OrderedDict([('weights', tensor([0.3367])), ('bias', tensor([0.1288]))])

# Setup a loss function
loss_fn = nn.L1Loss() --> L1Loss --> https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html

# Setup an optimizer (stochastic gradient descent)
optimizer = torch.optim.SGD(params=model_0.parameters(), # we want to optimize the parameters present in our model
                            lr=0.01) # lr = learning rate = possibly the most important hyperparameter you can set
     
Q: Which loss function and optimizer should I use?

A: This will be problem specific. But with experience, you'll get an idea of what works and what doesn't with your particular problem set.

For example, for a regression problem (like ours), a loss function of nn.L1Loss() and an optimizer like torch.optim.SGD() will suffice.

But for a classification problem like classifying whether a photo is of a dog or a cat, you'll likely want to use a loss function of nn.BCELoss() (binary cross entropy loss).

-----


Building a training loop (and a testing loop) in PyTorch

A couple of things we need in a training loop:

Loop through the data and do...
Forward pass (this involves data moving through our model's forward() functions) to make predictions on data - also called forward propagation
Calculate the loss (compare forward pass predictions to ground truth labels)
Optimizer zero grad
Loss backward - move backwards through the network to calculate the gradients of each of the parameters of our model with respect to the loss (backpropagation - https://www.youtube.com/watch?v=tIeHLnjs5U8)
Optimizer step - use the optimizer to adjust our model's parameters to try and improve the loss (gradient descent - https://youtu.be/IHZwWFHWa-w)

torch.manual_seed(42)

# An epoch is one loop through the data... (this is a hyperparameter because we've set it ourselves)
epochs = 200

# Track different values
epoch_count = [] 
loss_values = []
test_loss_values = [] 

### Training
# 0. Loop through the data
for epoch in range(epochs): 
  # Set the model to training mode
  model_0.train() # train mode in PyTorch sets all parameters that require gradients to require gradients 

  # 1. Forward pass
  y_pred = model_0(X_train)

  # 2. Calculate the loss
  loss = loss_fn(y_pred, y_train)

  # 3. Optimizer zero grad
  optimizer.zero_grad()    --> 5. adımdaki gradientleri sıfırlamazsak gradientler toplanır ya da başka bir şey olur. Böylelikle backpropogation evresinde parametreler yanlış ayarlanır.

  # 4. Perform backpropagation on the loss with respect to the parameters of the model (calculate gradients of each parameter)
  loss.backward()

  # 5. Step the optimizer (perform gradient descent)
  optimizer.step() # by default how the optimizer changes will accumulate through the loop so... we have to zero them above in step 3 for the next iteration of the loop

-------------------------------------------------------------------

  ### Testing

  model_0.eval() # turns off different settings in the model not needed for evaluation/testing (dropout/batch norm layers)
  with torch.inference_mode(): # turns off gradient tracking & a couple more things behind the scenes - https://twitter.com/PyTorch/status/1437838231505096708?s=20&t=aftDZicoiUGiklEP179x7A
  # with torch.no_grad(): # you may also see torch.no_grad() in older PyTorch code
    # 1. Do the forward pass 
    test_pred = model_0(X_test)             --> test verilerini kullandık.

    # 2. Calculate the loss                 --> modelin test kısmında modeli eğitmediğimiz için gradient decent ve back propogation uygulamıyoruz. Dolayısıyla da 3. 4. ve 5. adımlar yok.
    test_loss = loss_fn(test_pred, y_test)

  # Print out what's happenin'
  if epoch % 10 == 0:
    epoch_count.append(epoch)
    loss_values.append(loss)           --> grafikte göstermek için bütün loss değerlerini biriktirdik.
    test_loss_values.append(test_loss)
    print(f"Epoch: {epoch} | Loss: {loss} | Test loss: {test_loss}") --> loss değerlerini yazdık.
    # Print out model state_dict()
    print(model_0.state_dict())



Epoch: 0 | Loss: 0.3013603389263153 | Test loss: 0.4675942063331604
OrderedDict([('weights', tensor([0.3445])), ('bias', tensor([0.1488]))])
Epoch: 10 | Loss: 0.18615034222602844 | Test loss: 0.3328842222690582
OrderedDict([('weights', tensor([0.3835])), ('bias', tensor([0.2488]))])
Epoch: 20 | Loss: 0.08227583020925522 | Test loss: 0.2069590985774994
OrderedDict([('weights', tensor([0.4222])), ('bias', tensor([0.3403]))])
Epoch: 30 | Loss: 0.05181945487856865 | Test loss: 0.14023718237876892
OrderedDict([('weights', tensor([0.4539])), ('bias', tensor([0.3788]))])
Epoch: 40 | Loss: 0.04503796249628067 | Test loss: 0.11182951927185059
OrderedDict([('weights', tensor([0.4768])), ('bias', tensor([0.3868]))])
Epoch: 50 | Loss: 0.04132963344454765 | Test loss: 0.09809747338294983
OrderedDict([('weights', tensor([0.4956])), ('bias', tensor([0.3838]))])
Epoch: 60 | Loss: 0.03785243630409241 | Test loss: 0.08776430785655975
OrderedDict([('weights', tensor([0.5134])), ('bias', tensor([0.3783]))])
Epoch: 70 | Loss: 0.03441363573074341 | Test loss: 0.0794917643070221
OrderedDict([('weights', tensor([0.5306])), ('bias', tensor([0.3713]))])
Epoch: 80 | Loss: 0.030979642644524574 | Test loss: 0.07190609723329544
OrderedDict([('weights', tensor([0.5475])), ('bias', tensor([0.3638]))])
Epoch: 90 | Loss: 0.02754882536828518 | Test loss: 0.06363357603549957
OrderedDict([('weights', tensor([0.5647])), ('bias', tensor([0.3568]))])
Epoch: 100 | Loss: 0.024110013619065285 | Test loss: 0.05536102131009102
OrderedDict([('weights', tensor([0.5818])), ('bias', tensor([0.3498]))])
Epoch: 110 | Loss: 0.020677709951996803 | Test loss: 0.04777535796165466
OrderedDict([('weights', tensor([0.5988])), ('bias', tensor([0.3423]))])
Epoch: 120 | Loss: 0.017245199531316757 | Test loss: 0.0395028181374073
OrderedDict([('weights', tensor([0.6159])), ('bias', tensor([0.3353]))])
Epoch: 130 | Loss: 0.013806397095322609 | Test loss: 0.031230276450514793
OrderedDict([('weights', tensor([0.6331])), ('bias', tensor([0.3283]))])
Epoch: 140 | Loss: 0.010375778190791607 | Test loss: 0.023644620552659035
OrderedDict([('weights', tensor([0.6501])), ('bias', tensor([0.3208]))])
Epoch: 150 | Loss: 0.006941580679267645 | Test loss: 0.0153720797970891
OrderedDict([('weights', tensor([0.6672])), ('bias', tensor([0.3138]))])
Epoch: 160 | Loss: 0.0035027749836444855 | Test loss: 0.007099539041519165
OrderedDict([('weights', tensor([0.6844])), ('bias', tensor([0.3068]))])
Epoch: 170 | Loss: 0.0025885067880153656 | Test loss: 0.008447891101241112
OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])
Epoch: 180 | Loss: 0.0025885067880153656 | Test loss: 0.008447891101241112
OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])
Epoch: 190 | Loss: 0.0025885067880153656 | Test loss: 0.008447891101241112
OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])






import numpy as np
np.array(torch.tensor(loss_values).numpy()), test_loss_values

# Plot the loss curves                                                                 --> loss değerlerini grafiğe döktük.
plt.plot(epoch_count, np.array(torch.tensor(loss_values).numpy()), label="Train loss") --> loss_values direkt çalışmadı. Dolayısıyla bu şekilde yazdık. torch.tensor(loss_values) de 
                                                                                           burada doğru çalışıyor olabilir. Önemli olan tensörlerdeki grad_fn'leri ayırmak olabilir.
plt.plot(epoch_count, test_loss_values, label="Test loss")  --> önceki oluşturduğumuz grafikten farklı olarak burada eğri oluşturduk. Önceki grafikte veriler nokta şeklinde yayılmışlardı.
plt.title("Training and test loss curves") --> grafiğe isim verdik.
plt.ylabel("Loss")                         --> eksenlere isim verdik.
plt.xlabel("Epochs")
plt.legend();




plot_predictions(predictions=test_pred); --> 2. grafik oluşturduk. modelimizin tahminlerini inceledik. Bizim yaptığımız tahminler modelin training aşamasındaki 
                                            loop'ta gerçekleştiği için parametreler belki güncel olmayabilir. Dolayısıyla training kısmının bulunduğu loop dışarısında 
	                                    bir daha tahmin işlemi yapılabilir.
                                             
                                               


-----

Saving a model in PyTorch

There are three main methods you should about for saving and loading models in PyTorch.

torch.save() - allows you save a PyTorch object in Python's pickle format
torch.load() - allows you load a saved PyTorch object
torch.nn.Module.load_state_dict() - this allows to load a model's saved state dictionary
PyTorch save & load code tutorial + extra-curriculum - https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference


# Saving our PyTorch model

from pathlib import Path

# 1. Create models directory 
MODEL_PATH = Path("models")
MODEL_PATH.mkdir(parents=True, exist_ok=True)  --> If parents is true, any missing parents of this path are created as needed
                                                   Bu dosya el ile oluşturulabilir.

# 2. Create model save path
MODEL_NAME = "01_pytorch_workflow_model_0.pth"
MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME

# 3. Save the model state dict
print(f"Saving model to: {MODEL_SAVE_PATH}")
torch.save(obj=model_0.state_dict(),
           f=MODEL_SAVE_PATH)
     
Saving model to: models/01_pytorch_workflow_model_0.pth

Modeli download etmezsek colab silebilir.


-----

Loading a PyTorch model

Since we saved our model's state_dict() rather the entire model, we'll create a new instance of our model class and load the saved state_dict() into that.


# To load in a saved state_dict we have to instantiate a new instance of our model class
loaded_model_0 = LinearRegressionModel()

# Load the saved state_dict of model_0 (this will update the new instance with updated parameters)
loaded_model_0.load_state_dict(torch.load(f=MODEL_SAVE_PATH))
     
<All keys matched successfully>

loaded_model_0.state_dict()
     
OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])

# Make some predictions with our loaded model
loaded_model_0.eval()
with torch.inference_mode():
  loaded_model_preds = loaded_model_0(X_test)

loaded_model_preds
     
tensor([[0.8685],
        [0.8825],
        [0.8965],
        [0.9105],
        [0.9245],
        [0.9384],
        [0.9524],
        [0.9664],
        [0.9804],
        [0.9944]])

# Make some models preds
model_0.eval()
with torch.inference_mode():
  y_preds = model_0(X_test)

y_preds
     
tensor([[0.8685],
        [0.8825],
        [0.8965],
        [0.9105],
        [0.9245],
        [0.9384],
        [0.9524],
        [0.9664],
        [0.9804],
        [0.9944]])

# Compare loaded model preds with original model preds -->  Doğru save dosyasını mı load ettik. Onu kontrol ettik.
y_preds == loaded_model_preds
     
tensor([[True],
        [True],
        [True],
        [True],
        [True],
        [True],
        [True],
        [True],
        [True],
        [True]])

-----


Building a PyTorch Linear model

# Create a linear model by subclassing nn.Module
class LinearRegressionModelV2(nn.Module):
  def __init__(self):
    super().__init__()
    # Use nn.Linear() for creating the model parameters / also called: linear transform, probing layer, fully connected layer, dense layer
    self.linear_layer = nn.Linear(in_features=1,              --> feature sayısı bir olan input alır. Bu kısım içeri giren inputları temsil ediyor.
                                  out_features=1)             --> Burası dışarı çıkan outputu temsil ediyor. Linear ise gelen dataya linear regresyon formülü uyguluyor. 
    
  def forward(self, x: torch.Tensor) -> torch.Tensor:
    return self.linear_layer(x)


torch.manual_seed(42)

model_1 = LinearRegressionModelV2()  

model_1

LinearRegressionModelV2(
  (linear_layer): Linear(in_features=1, out_features=1, bias=True)
)

model_1.state_dict()
     
OrderedDict([('linear_layer.weight', tensor([[0.7645]])),
             ('linear_layer.bias', tensor([0.8300]))])



# Check the model current device
next(model_1.parameters()).device            --> modelin device'i --> device(type='cpu')
     


# Set the model to use the target device
model_1.to(device)                           --> modelin device'ini değiştirdik. 
next(model_1.parameters()).device            --> device(type='cuda', index=0) --> gpu'da çalıştırınca cuda yazıyor.
     
                
model_1.state_dict()                         
     
OrderedDict([('linear_layer.weight', tensor([[0.7645]], device='cuda:0')),
             ('linear_layer.bias', tensor([0.8300], device='cuda:0'))])



# Put data on the target device (device agnostic code for data)  --> datalar uygun device'e ayarlanmazsa kodlar hata verir.
X_train = X_train.to(device)
y_train = y_train.to(device)
X_test = X_test.to(device)
y_test = y_test.to(device)


# Check out our model predictions visually
plot_predictions(predictions=y_preds.cpu()) --> Grafik işlemleri cpu'da çalıştığı için dataları cpu'ya çevirdik.



# Put the loaded model to device
loaded_model_1.to(device) --> loadlamak için açılan yeni modelin cihazını değiştirdik.
     

next(loaded_model_1.parameters()).device --> device(type='cuda', index=0)
     


-------------------------------------------------------------------

Exercises & Extra-curriculum
For exercise & extra-curriculum, refer to: https://www.learnpytorch.io/01_pytorch_workflow/#exercises







	