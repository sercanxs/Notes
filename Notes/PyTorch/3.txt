import torch
from torch import nn            --> nn contains all of PyTorch's building blocks for neural networks 
import matplotlib.pyplot as plt

# Check PyTorch version
torch.__version__

----

1. Data (preparing and loading)
Data can be almost anything... in machine learning.

Excel speadsheet
Images of any kind
Videos (YouTube has lots of data...)
Audio like songs or podcasts
DNA
Text

Machine learning is a game of two parts:

Get data into a numerical representation.
Build a model to learn patterns in that numerical representation.


Linear regression

In statistics, linear regression is a statistical model that estimates the linear relationship between a scalar response (dependent variable) and 
one or more explanatory variables (regressor or independent variable). The case of one explanatory variable is called simple linear regression; 
for more than one, the process is called multiple linear regression.[1] This term is distinct from multivariate linear regression, 
where multiple correlated dependent variables are predicted, rather than a single scalar variable.[2] If the explanatory variables are measured 
with error then errors-in-variables models are required, also known as measurement error models.


Doğrusal regresyonda, karmaşık sayı setini bir doğru formülüne indirgemeye çalışılır. Buna göre tüm noktalara en yakından geçen bir doğru çizmek amaçlanır.
Bu hata miktarının en az tutulmasıdır. Hata, verilerdeki yanlışlık değil, sapmadır. Yanlış olan modeldir.
Doğrusal regresyon ile amaçlanan bu noktaların tamamına en yakın geçen doğruyu elde etmektir. Her doğrunun bir formülü olduğu gibi bu doğrunun da karakteristik
bir şekilde y = ax + b denklemine uygun bir formülü, bir (a,b) ikilisi bulunacaktır.

Kaynaklar = https://en.wikipedia.org/wiki/Linear_regression
	    https://aydemirhamza.medium.com/do%C4%9Frusal-lineer-regresyon-de873548e0fd





To showcase this, let's create some known data using the linear regression formula.
We'll use a linear regression formula to make a straight line with known parameters.


# Create *known* parameters
weight = 0.7
bias = 0.3

# Create
start = 0
end = 1
step = 0.02
X = torch.arange(start, end, step).unsqueeze(dim=1)
y = weight * X + bias                               --> y = bx + a  --> bu denklemde a ve b parametreler olabilir.

X[:10], y[:10]       --> 10. indexin solundakileri yazdı 10. index dahil değil.

Sonuç

(tensor([[0.0000],
         [0.0200],
         [0.0400],
         [0.0600],
         [0.0800],
         [0.1000],
         [0.1200],
         [0.1400],
         [0.1600],
         [0.1800]]),
 tensor([[0.3000],
         [0.3140],
         [0.3280],
         [0.3420],
         [0.3560],
         [0.3700],
         [0.3840],
         [0.3980],
         [0.4120],
         [0.4260]]))


len(X), len(y)  --> (50, 50)


-------------------------------------------------------------------

Splitting data into training and test sets (one of the most important concepts in machine learning in general)
Let's create a training and test set with our data.


# Create a train/test split
train_split = int(0.8 * len(X))
X_train, y_train = X[:train_split], y[:train_split]
X_test, y_test = X[train_split:], y[train_split:] 

len(X_train), len(y_train), len(X_test), len(y_test) --> (40, 40, 10, 10)


-----

How might we better visualize our data?


def plot_predictions(train_data=X_train,          = function oluşturduk.
                     train_labels=y_train,
                     test_data=X_test,
                     test_labels=y_test,
                     predictions=None):
  """
  Plots training data, test data and compares predictions.
  """
  plt.figure(figsize=(10, 7))  = grafiğin boyutunu ayarladık. Matplotlib'in özelliğini kullandık.

  # Plot training data in blue
  plt.scatter(train_data, train_labels, c="b", s=4, label="Training data") --> c, colour'i temsil ediyor. b ise blue'yi temsil ediyor. s, ise grafikte nokta olarak gösterilen verilerin 
									       boyutunu ayarlıyor. Bunun yanında ekstradan label da verdik.

  # Plot test data in green
  plt.scatter(test_data, test_labels, c="g", s=4, label="Testing data") --> train data'si yanında test datasını da ayarladık. colour'i ise g yani green yaptık.

  # Are there predictions?
  if predictions is not None:             = tahminimiz olmadığı için predictions değişkenini None olarak ayarlamıştık. eğer none olmasaydı predictions'i da grafikte gösterecektik.
    # Plot the predictions if they exist
    plt.scatter(test_data, predictions, c="r", s=4, label="Predictions")
  
  # Show the legend
  plt.legend(prop={"size": 14}); = yukarıda verilere label girmiştik ama bu label grafikte gösterilmiyordu. legend oluşturunca labeller, grafikte gösteriliyor. size label'in
                                   boyutunu belirtiyor. 
     

plot_predictions(); --> functionı çağırınca ekranda grafik oluştu.



-------------------------------------------------------------------

Our first PyTorch model!



Because we're going to be building classes throughout the course, I'd recommend getting familiar with OOP in Python, to do so 
you can use the following resource from Real Python: https://realpython.com/python3-object-oriented-programming/

What our model does:

Start with random values (weight & bias)
Look at training data and adjust the random values to better represent (or get closer to) the ideal 
values (the weight & bias values we used to create the data) 
How does it do so?

Through two main algorithms: PyTorch bunları arkada otomatik yapıyor.

Gradient descent - https://youtu.be/IHZwWFHWa-w
Backpropagation - https://youtu.be/Ilg3gGewQ5U

Gradient Descent, rastgele alınan değişkenlerle başlayarak global minimum değerine ulaşmayı amaçlar. Hesaplamalarında türevler var.


Backpropagation is a machine learning technique essential to the optimization of artificial neural networks. It facilitates the use
of gradient descent algorithms to update network weights, which is how the deep learning models driving modern artificial intelligence (AI) “learn.”
Abstractly speaking, the purpose of backpropagation is to train a neural network to make better predictions through supervised learning.
More fundamentally, the goal of backpropagation is to determine how model weights and biases should be adjusted to minimize error as measured by a "loss function".




from torch import nn --> nn'i import ettik.

# Create linear regression model class
class LinearRegressionModel(nn.Module): # <- almost everything in PyTorch inherhits from nn.Module
  def __init__(self):                   --> constructor
    super().__init__()
    self.weights = nn.Parameter(torch.randn(1, # <- start with a random weight and try to adjust it to the ideal weight    --> parametre oluşturduk.
                                            requires_grad=True, # <- can this parameter be updated via gradient descent? default=True.     
                                            dtype=torch.float)) # <- PyTorch loves the datatype torch.float32 (datatypelerin birden çok ismi olabilir.)
    
    self.bias = nn.Parameter(torch.randn(1, # <- start with a random bias and try to adjust it to the ideal bias
                                         requires_grad=True, # <- can this parameter be updated via gradient descent? 
                                         dtype=torch.float)) # <- PyTorch loves the datatype torch.float32 
    
  # Forward method to define the computation in the model
  def forward(self, x: torch.Tensor) -> torch.Tensor: # <- "x" is the input data --> x'in torch.Tensor olması gerekir.
    return self.weights * x + self.bias # this is the linear regression formula



x:torch.Tensor  =  means that the function expects x to be of type tensor.

-> torch.tensor =  means that the functions should return a value of type tensor.

-----


PyTorch model building essentials

torch.nn - contains all of the buildings for computational graphs (a neural network can be considered a computational graph)
torch.nn.Parameter - what parameters should our model try and learn, often a PyTorch layer from torch.nn will set these for us
torch.nn.Module - The base class for all neural network modules, if you subclass it, you should overwrite forward()
torch.optim - this where the optimizers in PyTorch live, they will help with gradient descent
def forward() - All nn.Module subclasses require you to overwrite forward(), this method defines what happens in the forward computation


See more of these essential modules via the PyTorch cheatsheet - https://pytorch.org/tutorials/beginner/ptcheat.html

-----

Checking the contents of our PyTorch model
Now we've created a model, let's see what's inside...

So we can check our model parameters or what's inside our model using .parameters().


# Create a random seed
torch.manual_seed(42)

# Create an instance of the model (this is a subclass of nn.Module)
model_0 = LinearRegressionModel()

# Check out the parameters
list(model_0.parameters())        --> modeldeki parametrelere bakmak için list içinde yazdık. 
     
[Parameter containing:
 tensor([0.3367], requires_grad=True),
 Parameter containing:
 tensor([0.1288], requires_grad=True)]

# List named parameters
model_0.state_dict()              --> bu şekilde yazınca parametreler isimleriyle birlikte geliyor.
     
OrderedDict([('weights', tensor([0.3367])), ('bias', tensor([0.1288]))])

-----



Making prediction using torch.inference_mode()


To check our model's predictive power, let's see how well it predicts y_test based on X_test.

When we pass data through our model, it's going to run it through the forward() method.


y_preds = model_0(X_test) --> modelimize verilerimizi ekledik. model verilerimizi çalıştırdı ve x'e göre y'i tahmin etti.
y_preds
     
tensor([[0.3982],
        [0.4049],
        [0.4116],
        [0.4184],
        [0.4251],
        [0.4318],
        [0.4386],
        [0.4453],
        [0.4520],
        [0.4588]], grad_fn=<AddBackward0>)

# Make predictions with model
with torch.inference_mode(): --> inference mode koymayınca yukarıdaki gibi grad_fn oluşuyor. inference mode bazı özellikleri kapatıyor ve model daha hızlı çalışıyor.
  y_preds = model_0(X_test)
  

# # You can also do something similar with torch.no_grad(), however, torch.inference_mode() is preferred
# with torch.no_grad():
#   y_preds = model_0(X_test)

y_preds
     
tensor([[0.3982],
        [0.4049],
        [0.4116],
        [0.4184],
        [0.4251],
        [0.4318],
        [0.4386],
        [0.4453],
        [0.4520],
        [0.4588]]) --> inference mode'de grad_fn oluşmadı.


See more on inference mode here - https://twitter.com/PyTorch/status/1437838231505096708?s=20&t=cnKavO9iTgwQ-rfri6u7PQ


y_test
     
tensor([[0.8600],
        [0.8740],
        [0.8880],
        [0.9020],
        [0.9160],
        [0.9300],
        [0.9440],
        [0.9580],
        [0.9720],
        [0.9860]]) --> önceden yaptığımız, parametreleri doğru ayarladığımız modelin tahmini (Belki bu model değildir.)


plot_predictions(predictions=y_preds) --> predictions'in default value'si None'du. Ona modelin yaptığı tahmini ekledik. grafikte gösterdik.


-------------------------------------------------------------------

Train model


The whole idea of training is for a model to move from some unknown parameters (these may be random) to some known parameters.

Or in other words from a poor representation of the data to a better representation of the data.

One way to measure how poor or how wrong your models predictions are is to use a loss function.

Note: Loss function may also be called cost function or criterion in different areas. For our case, we're going to refer to it as a loss function.


Things we need to train:

Loss function: A function to measure how wrong your model's predictions are to the ideal outputs, lower is better.
Optimizer: Takes into account the loss of a model and adjusts the model's parameters (e.g. weight & bias in our case) to improve the loss function - https://pytorch.org/docs/stable/optim.html#module-torch.optim
         Inside the optimizer you'll often have to set two parameters:
                       params - the model parameters you'd like to optimize, for example params=model_0.parameters()
                       lr (learning rate) - the learning rate is a hyperparameter that defines how big/small the optimizer changes the parameters with each step (a small lr results in small changes, a large lr results in large changes)

And specifically for PyTorch, we need:

A training loop
A testing loop

list(model_0.parameters())
     
[Parameter containing:
 tensor([0.3367], requires_grad=True),
 Parameter containing:
 tensor([0.1288], requires_grad=True)]

# Check out our model's parameters (a parameter is a value that the model sets itself)
model_0.state_dict()
     
OrderedDict([('weights', tensor([0.3367])), ('bias', tensor([0.1288]))])

# Setup a loss function
loss_fn = nn.L1Loss() --> L1Loss --> https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html

# Setup an optimizer (stochastic gradient descent)
optimizer = torch.optim.SGD(params=model_0.parameters(), # we want to optimize the parameters present in our model
                            lr=0.01) # lr = learning rate = possibly the most important hyperparameter you can set
     
Q: Which loss function and optimizer should I use?

A: This will be problem specific. But with experience, you'll get an idea of what works and what doesn't with your particular problem set.

For example, for a regression problem (like ours), a loss function of nn.L1Loss() and an optimizer like torch.optim.SGD() will suffice.

But for a classification problem like classifying whether a photo is of a dog or a cat, you'll likely want to use a loss function of nn.BCELoss() (binary cross entropy loss).

-----


Building a training loop (and a testing loop) in PyTorch

A couple of things we need in a training loop:

Loop through the data and do...
Forward pass (this involves data moving through our model's forward() functions) to make predictions on data - also called forward propagation
Calculate the loss (compare forward pass predictions to ground truth labels)
Optimizer zero grad
Loss backward - move backwards through the network to calculate the gradients of each of the parameters of our model with respect to the loss (backpropagation - https://www.youtube.com/watch?v=tIeHLnjs5U8)
Optimizer step - use the optimizer to adjust our model's parameters to try and improve the loss (gradient descent - https://youtu.be/IHZwWFHWa-w)

torch.manual_seed(42)

# An epoch is one loop through the data... (this is a hyperparameter because we've set it ourselves)
epochs = 200

# Track different values
epoch_count = [] 
loss_values = []
test_loss_values = [] 

### Training
# 0. Loop through the data
for epoch in range(epochs): 
  # Set the model to training mode
  model_0.train() # train mode in PyTorch sets all parameters that require gradients to require gradients 

  # 1. Forward pass
  y_pred = model_0(X_train)

  # 2. Calculate the loss
  loss = loss_fn(y_pred, y_train)

  # 3. Optimizer zero grad
  optimizer.zero_grad()    --> 5. adımdaki gradientleri sıfırlamazsak gradientler toplanır ya da başka bir şey olur. Böylelikle backpropogation evresinde parametreler yanlış ayarlanır.

  # 4. Perform backpropagation on the loss with respect to the parameters of the model (calculate gradients of each parameter)
  loss.backward()

  # 5. Step the optimizer (perform gradient descent)
  optimizer.step() # by default how the optimizer changes will accumulate through the loop so... we have to zero them above in step 3 for the next iteration of the loop

-------------------------------------------------------------------

  ### Testing









	