import torch
from torch import nn            --> nn contains all of PyTorch's building blocks for neural networks 
import matplotlib.pyplot as plt

# Check PyTorch version
torch.__version__

----

1. Data (preparing and loading)
Data can be almost anything... in machine learning.

Excel speadsheet
Images of any kind
Videos (YouTube has lots of data...)
Audio like songs or podcasts
DNA
Text

Machine learning is a game of two parts:

Get data into a numerical representation.
Build a model to learn patterns in that numerical representation.


Linear regression

In statistics, linear regression is a statistical model that estimates the linear relationship between a scalar response (dependent variable) and 
one or more explanatory variables (regressor or independent variable). The case of one explanatory variable is called simple linear regression; 
for more than one, the process is called multiple linear regression.[1] This term is distinct from multivariate linear regression, 
where multiple correlated dependent variables are predicted, rather than a single scalar variable.[2] If the explanatory variables are measured 
with error then errors-in-variables models are required, also known as measurement error models.


Doğrusal regresyonda, karmaşık sayı setini bir doğru formülüne indirgemeye çalışılır. Buna göre tüm noktalara en yakından geçen bir doğru çizmek amaçlanır.
Bu hata miktarının en az tutulmasıdır. Hata, verilerdeki yanlışlık değil, sapmadır. Yanlış olan modeldir.
Doğrusal regresyon ile amaçlanan bu noktaların tamamına en yakın geçen doğruyu elde etmektir. Her doğrunun bir formülü olduğu gibi bu doğrunun da karakteristik
bir şekilde y = ax + b denklemine uygun bir formülü, bir (a,b) ikilisi bulunacaktır.

Kaynaklar = https://en.wikipedia.org/wiki/Linear_regression
	    https://aydemirhamza.medium.com/do%C4%9Frusal-lineer-regresyon-de873548e0fd





To showcase this, let's create some known data using the linear regression formula.
We'll use a linear regression formula to make a straight line with known parameters.


# Create *known* parameters
weight = 0.7
bias = 0.3

# Create
start = 0
end = 1
step = 0.02
X = torch.arange(start, end, step).unsqueeze(dim=1)
y = weight * X + bias                               --> y = bx + a  --> bu denklemde a ve b parametreler olabilir.

X[:10], y[:10]       --> 10. indexin solundakileri yazdı 10. index dahil değil.

Sonuç

(tensor([[0.0000],
         [0.0200],
         [0.0400],
         [0.0600],
         [0.0800],
         [0.1000],
         [0.1200],
         [0.1400],
         [0.1600],
         [0.1800]]),
 tensor([[0.3000],
         [0.3140],
         [0.3280],
         [0.3420],
         [0.3560],
         [0.3700],
         [0.3840],
         [0.3980],
         [0.4120],
         [0.4260]]))


len(X), len(y)  --> (50, 50)


-------------------------------------------------------------------

Splitting data into training and test sets (one of the most important concepts in machine learning in general)
Let's create a training and test set with our data.


# Create a train/test split
train_split = int(0.8 * len(X))
X_train, y_train = X[:train_split], y[:train_split]
X_test, y_test = X[train_split:], y[train_split:] 

len(X_train), len(y_train), len(X_test), len(y_test) --> (40, 40, 10, 10)


-----

How might we better visualize our data?


def plot_predictions(train_data=X_train,          = function oluşturduk.
                     train_labels=y_train,
                     test_data=X_test,
                     test_labels=y_test,
                     predictions=None):
  """
  Plots training data, test data and compares predictions.
  """
  plt.figure(figsize=(10, 7))  = grafiğin boyutunu ayarladık. Matplotlib'in özelliğini kullandık.

  # Plot training data in blue
  plt.scatter(train_data, train_labels, c="b", s=4, label="Training data") --> c, colour'i temsil ediyor. b ise blue'yi temsil ediyor. s, ise grafikte nokta olarak gösterilen verilerin 
									       boyutunu ayarlıyor. Bunun yanında ekstradan label da verdik.

  # Plot test data in green
  plt.scatter(test_data, test_labels, c="g", s=4, label="Testing data") --> train data'si yanında test datasını da ayarladık. colour'i ise g yani green yaptık.

  # Are there predictions?
  if predictions is not None:             = tahminimiz olmadığı için predictions değişkenini None olarak ayarlamıştık. eğer none olmasaydı predictions'i da grafikte gösterecektik.
    # Plot the predictions if they exist
    plt.scatter(test_data, predictions, c="r", s=4, label="Predictions")
  
  # Show the legend
  plt.legend(prop={"size": 14}); = yukarıda verilere label girmiştik ama bu label grafikte gösterilmiyordu. legend oluşturunca labeller, grafikte gösteriliyor. size label'in
                                   boyutunu belirtiyor. 
     

plot_predictions(); --> functionı çağırınca ekranda grafik oluştu.



-------------------------------------------------------------------

Our first PyTorch model!



Because we're going to be building classes throughout the course, I'd recommend getting familiar with OOP in Python, to do so 
you can use the following resource from Real Python: https://realpython.com/python3-object-oriented-programming/

What our model does:

Start with random values (weight & bias)
Look at training data and adjust the random values to better represent (or get closer to) the ideal 
values (the weight & bias values we used to create the data) 
How does it do so?

Through two main algorithms: PyTorch bunları arkada otomatik yapıyor.

Gradient descent - https://youtu.be/IHZwWFHWa-w
Backpropagation - https://youtu.be/Ilg3gGewQ5U

Gradient Descent, rastgele alınan değişkenlerle başlayarak global minimum değerine ulaşmayı amaçlar. Hesaplamalarında türevler var.


Backpropagation is a machine learning technique essential to the optimization of artificial neural networks. It facilitates the use
of gradient descent algorithms to update network weights, which is how the deep learning models driving modern artificial intelligence (AI) “learn.”
Abstractly speaking, the purpose of backpropagation is to train a neural network to make better predictions through supervised learning.
More fundamentally, the goal of backpropagation is to determine how model weights and biases should be adjusted to minimize error as measured by a "loss function".




from torch import nn --> nn'i import ettik.

# Create linear regression model class
class LinearRegressionModel(nn.Module): # <- almost everything in PyTorch inherhits from nn.Module
  def __init__(self):                   --> constructor
    super().__init__()
    self.weights = nn.Parameter(torch.randn(1, # <- start with a random weight and try to adjust it to the ideal weight    --> parametre oluşturduk.
                                            requires_grad=True, # <- can this parameter be updated via gradient descent? default=True.     
                                            dtype=torch.float)) # <- PyTorch loves the datatype torch.float32 (datatypelerin birden çok ismi olabilir.)
    
    self.bias = nn.Parameter(torch.randn(1, # <- start with a random bias and try to adjust it to the ideal bias
                                         requires_grad=True, # <- can this parameter be updated via gradient descent? 
                                         dtype=torch.float)) # <- PyTorch loves the datatype torch.float32 
    
  # Forward method to define the computation in the model
  def forward(self, x: torch.Tensor) -> torch.Tensor: # <- "x" is the input data --> x'in torch.Tensor olması gerekir.
    return self.weights * x + self.bias # this is the linear regression formula



x:torch.Tensor  =  means that the function expects x to be of type tensor.

-> torch.tensor =  means that the functions should return a value of type tensor.

-----


PyTorch model building essentials

torch.nn - contains all of the buildings for computational graphs (a neural network can be considered a computational graph)
torch.nn.Parameter - what parameters should our model try and learn, often a PyTorch layer from torch.nn will set these for us
torch.nn.Module - The base class for all neural network modules, if you subclass it, you should overwrite forward()
torch.optim - this where the optimizers in PyTorch live, they will help with gradient descent
def forward() - All nn.Module subclasses require you to overwrite forward(), this method defines what happens in the forward computation


See more of these essential modules via the PyTorch cheatsheet - https://pytorch.org/tutorials/beginner/ptcheat.html

-----

Checking the contents of our PyTorch model
Now we've created a model, let's see what's inside...

So we can check our model parameters or what's inside our model using .parameters().


# Create a random seed
torch.manual_seed(42)

# Create an instance of the model (this is a subclass of nn.Module)
model_0 = LinearRegressionModel()

# Check out the parameters
list(model_0.parameters())        --> modeldeki parametrelere bakmak için list içinde yazdık. 
     
[Parameter containing:
 tensor([0.3367], requires_grad=True),
 Parameter containing:
 tensor([0.1288], requires_grad=True)]

# List named parameters
model_0.state_dict()              --> bu şekilde yazınca parametreler isimleriyle birlikte geliyor.
     
OrderedDict([('weights', tensor([0.3367])), ('bias', tensor([0.1288]))])

-----



Making prediction using torch.inference_mode()


To check our model's predictive power, let's see how well it predicts y_test based on X_test.

When we pass data through our model, it's going to run it through the forward() method.


y_preds = model_0(X_test) --> modelimize verilerimizi ekledik. model verilerimizi çalıştırdı ve x'e göre y'i tahmin etti.
y_preds
     
tensor([[0.3982],
        [0.4049],
        [0.4116],
        [0.4184],
        [0.4251],
        [0.4318],
        [0.4386],
        [0.4453],
        [0.4520],
        [0.4588]], grad_fn=<AddBackward0>)

# Make predictions with model
with torch.inference_mode(): --> inference mode koymayınca yukarıdaki gibi grad_fn oluşuyor. inference mode bazı özellikleri kapatıyor ve model daha hızlı çalışıyor.
  y_preds = model_0(X_test)
  

# # You can also do something similar with torch.no_grad(), however, torch.inference_mode() is preferred
# with torch.no_grad():
#   y_preds = model_0(X_test)

y_preds
     
tensor([[0.3982],
        [0.4049],
        [0.4116],
        [0.4184],
        [0.4251],
        [0.4318],
        [0.4386],
        [0.4453],
        [0.4520],
        [0.4588]]) --> inference mode'de grad_fn oluşmadı.


See more on inference mode here - https://twitter.com/PyTorch/status/1437838231505096708?s=20&t=cnKavO9iTgwQ-rfri6u7PQ


y_test
     
tensor([[0.8600],
        [0.8740],
        [0.8880],
        [0.9020],
        [0.9160],
        [0.9300],
        [0.9440],
        [0.9580],
        [0.9720],
        [0.9860]]) --> önceden yaptığımız, parametreleri doğru ayarladığımız modelin tahmini (Belki bu model değildir.)


plot_predictions(predictions=y_preds) --> predictions'in default value'si None'du. Ona modelin yaptığı tahmini ekledik. grafikte gösterdik.


-------------------------------------------------------------------





	